stages:
  - prepare
  - build
  - test
  - publish
  - deploy

services:
  - name: docker:dind
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
       cp $CLUSTER_CA /usr/local/share/ca-certificates/ && \
       update-ca-certificates && \
       exec /usr/local/bin/dockerd-entrypoint.sh

variables:
   DOCKER_HOST: tcp://localhost:2375/
   DOCKER_DRIVER: overlay2
   CLUSTER_CA: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

.kaniko:
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  before_script:
    - |-
      echo "{
          \"auths\": {
              \"$CI_REGISTRY\" :{
                  \"username\":\"$CI_REGISTRY_USER\",
                  \"password\":\"$CI_REGISTRY_PASSWORD\"}
               }
           }" > /kaniko/.docker/config.json
    - cat $CLUSTER_CA >> /kaniko/ssl/certs/ca-certificates.crt
  script:
    - |-
      /kaniko/executor \
        --context $CI_PROJECT_DIR \
        --dockerfile $CI_PROJECT_DIR/Dockerfile \
        --destination $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA

prepare:sdk:
  stage: prepare
  extends: .kaniko
  script:
    - |-
      /kaniko/executor \
        --context $CI_PROJECT_DIR \
        --dockerfile $CI_PROJECT_DIR/scripts/Dockerfile.sdk \
        --destination $CI_REGISTRY_IMAGE:sdk
  only:
    changes:
      - scripts/Dockerfile.sdk
      - paket.dependencies
      - paket.lock

build:
  stage: build
  image:
    name: $CI_REGISTRY_IMAGE:sdk
  before_script:
    - mono .paket/paket.exe restore
  script:
    - dotnet restore build.proj
    - dotnet fake build
    - dotnet publish -c Release -o deploy src/Server/Server.fsproj
    - cp -r src/Client/deploy deploy/public
  artifacts:
    paths:
      - deploy/
    expire_in: 2 hrs

test:debug:
  stage: test
  image:
    name: mcr.microsoft.com/dotnet/core/aspnet:2.2-alpine3.9
  variables:
      GIT_STRATEGY: none
      PRODUCTION: 1
      SERVER_PORT: 80
  script:
    - cd deploy
    - dotnet Server.dll &
    - sleep 30
    - kill %1
  dependencies:
    - build

publish:release:
  stage: publish
  extends: .kaniko
  dependencies:
    - build

# adding an environment and kubernetes cluster to the project gives
# access to the KUBE_* envirnonment variables
# you must install tiller in $KUBE_NAMESPACE before deploying:
# $ helm init --tiller-namespace ${KUBE_NAMESPACE} --service-account ${KUBE_NAMESPACE}-service-account
.deploy:
  image: registry.itpartner.no/innovasjon/warmup:latest
  stage: deploy
  variables:
      GIT_STRATEGY: none
      TILLER_NAMESPACE: $KUBE_NAMESPACE
  before_script:
    - |
      cp $CLUSTER_CA /usr/local/share/ca-certificates/
      update-ca-certificates
      docker login -u $CI_REGISTRY_USER -p $CI_JOB_TOKEN $CI_REGISTRY
      kubectl config set-cluster default --server="$KUBE_URL"
      kubectl config set-credentials admin --token="$KUBE_TOKEN"
      kubectl config set-context default --cluster=default --user=admin
      kubectl config use-context default
    # - helm init --upgrade && sleep 20
  script:
    - helm list
  dependencies: []

deploy:staging:
  extends: .deploy
  environment:
      name: staging
  script:
    - docker pull $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA
    - docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA $CI_REGISTRY_IMAGE:staging
    - docker push $CI_REGISTRY_IMAGE:staging
  when: manual

deploy:production:
  extends: .deploy
  environment:
      name: production
  script:
    - docker pull $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA
    - docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA $CI_REGISTRY_IMAGE:latest
    - docker push $CI_REGISTRY_IMAGE:latest
    # - kubectl delete pods -n default -lapp=warmup
  when: manual

